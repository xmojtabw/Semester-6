\documentclass{article}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{bytefield}
\usepackage{caption}
\usepackage{forest}
\usepackage{fancyhdr} % Custom headers/footers
\usepackage{colortbl}
\usepackage[left=0.6in, right=0.6in, top=1in, bottom=0.9in]{geometry}
\usepackage{indentfirst}
\usepackage{changepage, titlesec}
\usepackage{booktabs}
\usepackage{array}
\usepackage{adjustbox} % for adjustwidth
\usepackage{multicol} % for side-by-side columns
\setlength{\parindent}{1.5em} % Set indentation size (optional)
\usepackage{amsmath} % Required for align environment
\usepackage{xepersian}
\settextfont{Vazirmatn FD}
\setlatintextfont{Noto Serif} 


\pagestyle{fancy}     % Enable fancy headers
\fancyhf{}            % Clear default header/footer
\renewcommand{\headrulewidth}{0pt} % Disable default header line

\fancyhead[L]{\rule{\textwidth}{1pt}} % Manually add one line
\fancyfoot[C]{\thepage} % Page number in the center of the footer

\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}
\definecolor{lightcyan}{rgb}{0.84,1,1}
\definecolor{lightgreen}{rgb}{0.64,1,0.71}
\definecolor{lightred}{rgb}{1,0.7,0.71}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\ExplSyntaxOn
\NewDocumentCommand{\ReverseWords}{m}
{
	\seq_set_split:Nnn \l_tmpa_seq { ~ } { #1 } % Split words by spaces
	\seq_reverse:N \l_tmpa_seq % Reverse the order of words
	\seq_use:Nn \l_tmpa_seq { ~ } % Join words with spaces and output
}
\ExplSyntaxOff


\begin{document}
	\author{ مجتبی ملائی \\ ۴۰۱۳۱۳۸۳ }
	\title{ \huge {\textbf{ تکلیف اول}}}
	\date{}
	\maketitle
	
\section{}
\begin{enumerate}
	\item 
		اگر توابع فعال ساز را حذف کنیم، فرمول نهایی به شکل زیر خواهد بود:
		
		$$
		\hat{y} = \sum_{j=1}^{4} ( \sum_{i=1}^{3} x_i w_{ij}^{(1)} + b_j^{(1)} )w_{i}^{(2)} + b_{j}^{(2)}
		$$
		که میتوان آن را به صورت زیر نوشت:
		$$
		\hat{y} = \sum_{j=1}^{4} ( \sum_{i=1}^{3} x_i w_{ij}^{(1)}w_{j}^{(2)} + b_j^{(1)}w_{j}^{(2)} ) + b_{j}^{(2)}
		$$
		$$
		\hat{y} = \sum_{i=1}^{3} ( x_i ( \sum_{j=1}^{4}  w_{ij}^{(1)}w_{j}^{(2)}) + \sum_{j=1}^{4}(  b_j^{(1)}w_{j}^{(2)} )) + \sum_{j=1}^{4} b_{j}^{(2)}
		$$
		حالا اگر قرار دهیم:
		$$
		w'_i =  \sum_{j=1}^{4}  w_{ij}^{(1)}w_{j}^{(2)},
		b' = \sum_{j=1}^{4}  b_j^{(1)}w_{j}^{(2)} + b_{j}^{(2)}
		$$
		خواهیم داشت:
		$$
		\hat{y} = \sum_{i=1}^{3} x_i w'_i + b' 
		$$
		که در واقع همان ساختار یک شبکه عصبی بدون لایه پنهان است.
		
		
\item 

	\begin{enumerate}[label=\textbf{\Alph*)}]
		\item \( \frac{\partial \mathcal{L}}{\partial \hat{y}} =  -\frac{y}{\hat{y}} + \frac{(1-y)}{1-\hat{y}}  \)
		
		\item  \( \frac{\partial \hat{y}}{\partial z^{(2)}}= \sigma(z^{(2)}) (1-\sigma(z^{(2)})  ) = \hat{y}(1-\hat{y}) \)
		
		\item \( \frac{\partial a^{(1)}_j}{\partial z^{(1)}_j} =
		 1-\tanh^2(z^{(1)}_j) = 1 - (a^{(1)}_j)^2 \)
		
		\item \(  \frac{\partial J}{\partial z^{(2)}} =
		\frac{\partial J}{\partial \mathcal{L}} 
				\frac{\partial \mathcal{L}}{\partial \hat{y}}
				\frac{\partial \hat{y} }{ \partial z^{(2)}}
				= \frac{1}{m} ( -\frac{y}{\hat{y}} + \frac{(1-y)}{1-\hat{y}})
				\hat{y}(1-\hat{y}) = \frac{1}{m} ( \hat{y} -y )
		\)
		
		\item \( \frac{\partial \mathcal{L}}{\partial w^{(1)}_{ij}} = \frac{\partial \mathcal{L}}{\partial  z^{(2)} }
\frac{\partial z^{(2)} }{\partial a^{(1)}_j} 
\frac{\partial a^{(1)}_j }{\partial z^{(1)}_j} 
\frac{\partial z^{(1)}_j }{\partial w^{(1)}_{ij}} 
= (\hat{y} - y ) (w^{(2)}_j) (1 - (a^{(1)}_j)^2) (x^{(1)}_i)
\)
		
		\item \( \frac{\partial \mathcal{L}}{\partial b^{(1)}_{j}} = \frac{\partial \mathcal{L}}{\partial  z^{(2)} }
		\frac{\partial z^{(2)} }{\partial a^{(1)}_j} 
		\frac{\partial a^{(1)}_j }{\partial z^{(1)}_j} 
		\frac{\partial z^{(1)}_j }{\partial b^{(1)}_{j}}
		= (\hat{y} - y ) (w^{(2)}_j) (1 - (a^{(1)}_j)^2) 
		\)
		
		\item \( \frac{\partial \mathcal{L}}{\partial w^{(2)}_j} = \frac{\partial \mathcal{L}}{\partial z^{(2)}}
		\frac{\partial z^{(2)} }{\partial w^{(2)}_j} 
		= (\hat{y} - y )a^{(1)}_j
		\)
		
		\item \( \frac{\partial \mathcal{L}}{\partial b^{(2)} } = 
		\frac{\partial \mathcal{L}}{\partial z^{(2)}}
		\frac{\partial z^{(2)} }{\partial b^{(2)}} 
		= \hat{y} - y 
		\)
	\end{enumerate}
\vspace{3cm}
\item 
 ‎  
\begin{latin}
	\textbf{forward propagation:}
	

	$$z^{(1)}_j = [-0.1425,  0.1036,  0.7293, -0.1154] $$
	$$a^{(1)}_j = [-0.14154322,  0.10323094,  0.62263691, -0.11489045]$$ 
	$$ z^{(2)} = 0.31613438,\hat{y} = 0.57838188,  \mathcal{L} = 0.54752093 , J = 0.54752093 $$
	
	\textbf{back-propagation:}
	$$
	\frac{\partial J}{\partial w^{(2)}_j} =
	\begin{bmatrix}
			-0.05967719\\  0.04352403\\  0.262515  \\ -0.04843989
	\end{bmatrix}
	,
		\frac{\partial J}{\partial b^{(2)} } = 0.42161812
	$$
	$$ \frac{\partial J}{\partial w^{(1)}_{ij}} =
	\begin{bmatrix}
		 0.02375734& -0.03357857&  0.00890675&  0.00478461 \\
		 0.04648176& -0.0656972 &  0.01742624 & 0.00936119 \\
		 0.06920618& -0.09781583&  0.02594574 & 0.01393777 
	\end{bmatrix}
	,
		 \frac{\partial J}{\partial b^{(1)}_{j}} = 
	\begin{bmatrix}
		0.1032928 \\ -0.14599378 \\  0.03872499\\  0.02080264
	\end{bmatrix}
	 $$
\textbf{updated parameters:}
	$$
	w^{(2)}_j = w^{(2)}_j - 0.01 * \frac{\partial J}{\partial w^{(2)}_j}=
	\begin{bmatrix}
		0.25059677 \\ -0.35043524 \\  0.14737485 \\  0.0504844
	\end{bmatrix}
	,b^{(2)}  = b^{(2)} - 0.01 * \frac{\partial J}{\partial b^{(2)} } = 0.29578382
	$$
	$$
	w^{(1)}_{ij} = w^{(1)}_{ij}  - 0.01 *  \frac{\partial J}{\partial w^{(1)}_{ij}} 
=\begin{bmatrix}
 0.44976243& -0.11966421&  0.77991093&  0.71995215 \\
 0.04953518&  0.35065697& -0.22017426& -0.85009361 \\
-0.55069206&  0.11097816&  0.66974054&  0.44986062
\end{bmatrix} $$
$$
 b^{(1)}_{j} =  b^{(1)}_{j} - 0.01 * \frac{\partial J}{\partial b^{(1)}_{j}} = 
 \begin{bmatrix}
 	  0.09896707 \\ -0.09854006 \\  0.19961275 \\ -0.20020803
 \end{bmatrix}
	$$

\end{latin}
\end{enumerate}

\section{}
	میتوانیم از نمونه‌برداری مجدد داده‌ها (Resampling) به دو صورت زیر استفاده کنیم
	 \begin{itemize}
	 	\item 
	 	
	 	\textbf{Oversampling (افزایش داده‌های اقلیت):} می‌توان داده‌های دارای برچسب ۰ را با تکرار داده‌های موجود یا ایجاد داده‌های مصنوعی افزایش داد.
	 	
	 	\item 
	 	\textbf{Undersampling (کاهش داده‌های اکثریت):} می‌توان تعداد نمونه‌های دارای برچسب ۱ را کاهش داد تا تعادل برقرار شود.
	 \end{itemize}
	
\vspace{6cm}
\section{}
	\begin{enumerate}
		\item ‎  
				
\begin{latin}
	
	\begin{center}
		\begin{minipage}{0.45\textwidth}
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				step & x  & \(f(x)\) & \(f'(x)\)  \\
				\hline
				0    & 5.5    & -7175   & 3867  \\
				\hline
				1    & 9.367 & -26994  & -5876 \\
				\hline
				2    & 15.244 & -53615  & -1345 \\
				\hline
				3    & 16.589 & -53654  & 1409  \\
				\hline
				4    & 15.180 & -53525  & -1459 \\
				\hline
				5    & 16.639 & -53582  & 1525  \\
				\hline
			\end{tabular}
			\captionof{table}{ $\alpha = 0.001$}
		\end{minipage}
		\hspace{1em} % Adds space between tables
		\begin{minipage}{0.45\textwidth}
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				step & x  & \(f(x)\) & \(f'(x)\)  \\
				\hline
				0    & 5.5  & -7175   & -3867  \\
				\hline
				1    & 15.556  & -53946  & -772   \\
				\hline
				2    & 17.564  & -51109  & 3880   \\
				\hline
				3    & 7.476   & -16320  & -5272  \\
				\hline
				4    & 21.185  & -15350  & 16939  \\
				\hline
				5    & -22.858 & -197967 & -7377  \\
				\hline
			\end{tabular}
			\captionof{table}{ $\alpha = 0.0026$}
		\end{minipage}
	\end{center}
\end{latin}

\item  ‎  ‎ 
	\begin{itemize}
\item \textbf{نرخ یادگیری:}  
نرخ یادگیری بر یافتن کمینه جهانی \ReverseWords{(global minimum)} و سرعت همگرایی تأثیر دارد. مقدار زیاد آن باعث همگرایی سریع‌تر و جستجوی گسترده‌تر می‌شود، اما ممکن است نوسان ایجاد کرده و از کمینه عبور کند. مقدار کم نیز سرعت همگرایی را کاهش داده و ممکن است مدل را در کمینه محلی گرفتار کند.  

\item \textbf{تابع هزینه:}  
تابع هزینه با داشتن مینیمم‌های محلی یا فلات‌های زیاد می‌تواند روند یادگیری را دشوار کند و بر همگرایی تأثیر منفی بگذارد.  

\item \textbf{نقطه شروع:}  
انتخاب مناسب نقطه شروع می‌تواند مسیر مدل را به سمت نقطه بهینه هدایت کند، در حالی که نقطه شروع نامناسب ممکن است باعث کاهش سرعت همگرایی و گیر افتادن در کمینه محلی شود. 

	\end{itemize}
	
\item 
انتخاب یک نرخ یادگیری مناسب شرط \textbf{لازم} برای رسیدن به نقطه بهینه است، اما \textbf{به‌تنهایی کافی نیست}. رسیدن به نقطه بهینه علاوه بر نرخ یادگیری، به \textbf{تابع هزینه} و \textbf{مقادیر اولیه }پارامترها نیز بستگی دارد.
به عنوان مثال تابع زیر را در نظر بگیرید. اگر نقطه شروع در نزدیکی نقطه بهینه نباشد، به سمت کمینه های محلی میرود. اگر بخواهیم که از کمینه محلی خارج شویم باید نرخ یادگیری را تا حد زیادی افزایش دهیم که این موضوع نیز باعث میشود تا با تغییر نقطه آِغازین این امکان ایجاد شود که هر بار از روی نقطه بهینه پرش کنیم.
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{screenshot001}

	\label{fig:screenshot001}
\end{figure}

	\end{enumerate}
\vspace{6cm}
\section{}

\begin{enumerate}
	\item \textbf{ \ReverseWords{One-vs-Rest (OvR) Approach (Binary Logistic Regression for Each Class)} }
	میتوان از سه مدل \ReverseWords{logistic regression} استفاده کرد. برای اینکار برای هر کلاس یک مدل میسازیم که تشخیص دهد ار آن کلاس است یا از دو کلاس دیگر. به عنوان مثال خروجی مدل اول نشان میدهد که ورودی از نوع اول است یا نوع دوم و سوم است  \textbf{\(   (0,(1,2))\)}. در نهایت سه مدل خواهیم داشت و برای پیشبینی، کلاسی که بیشترین امتیاز را دارد(خروجی مدل آن بزرگ تر است) را انتخاب می‌کنیم. 
	\item 
	\textbf{\ReverseWords{Multinomial Logistic Regression (Softmax Regression)}}
	برای این کار نورون لایه خروجی را حذف میکنیم و بجای آن سه(در حالت کلی به تعداد کلاس های مورد نیاز) نورن قرار میدهیم. سپس خروجی هر نورون را به تابع softmax مدهیم و نهایتا کلاسی که بزرگترین خروجی را دارد را انتخاب میکنیم .
	\begin{figure}[!h]
		\centering
		\includegraphics[width=0.7\linewidth]{screenshot002}
		\label{fig:screenshot002}
	\end{figure}
	\vspace{0.5cm}
	
	ضابطه تابع softmax برای کلاس $i$ ام:
	$$
	\sigma(\vec{z})_i = \frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}} } 
	$$
	که در آن $K$ تعداد کلاس ها است.

 \item \textbf{ \ReverseWords{categorical cross-entropy}}
 میتوانیم از تابع زیر استفاده کنیم: 
 $$
  J = -\frac{1}{m} \sum_{i=0}^{m} \sum_{c=0}^{C} y_{i,c} \log(\sigma(\vec{z_i})_c)
 $$
 \begin{itemize}
 	\item اگر مقدار پیشبینی شده برای کلاس پاسخ ($ y_{i,c}=1$) نزدیک به یک باشد:
 	$\log(\sigma(\vec{z_i})_c)$ به سمت ۰ میل میکند پس هزینه را کم میکند که درست است. 
 	
 	\item اگر مقدار پیشبینی شده برای کلاس پاسخ ($ y_{i,c}=1$) نزدیک به صفر باشد:
$\log(\sigma(\vec{z_i})_c)$ به سمت  $ - \infty$ میل میکند پس هزینه را زیاد میکند که درست است. 
 	\item
 	 تابع softmax باعث  میشود تا با افزایش احتمال یک کلاس، شانس کلاس های دیگر کم بشود. پس برای وقتی که $y_{i,c}=0 $  اگر تابع عددی نزدیک به یک پیشبینی کرده باشد، هزینه به صورت غیر مستقیم زیاد می‌شود. 
 	 \item
 	  این تابع smooth و convex است. پس برای گرادیان کاهشی و بهینه ساز ها مناسب است.
 \end{itemize}
\end{enumerate}

\section{}


\end{document}
