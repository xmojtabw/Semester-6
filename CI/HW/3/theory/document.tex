\documentclass{article}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{bytefield}
\usepackage{array}
\usepackage{forest}
\usepackage{float}
\usepackage{fancyhdr} % Custom headers/footers
\usepackage{colortbl}
\usepackage[left=0.6in, right=0.6in, top=1in, bottom=0.9in]{geometry}
\usepackage{indentfirst}
\usepackage{changepage, titlesec}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{adjustbox} % for adjustwidth
\usepackage{multicol} % for side-by-side columns
\setlength{\parindent}{1.5em} % Set indentation size (optional)
\usepackage{amsmath} % Required for align environment
\usepackage{xepersian}
\settextfont{Vazirmatn FD}
\setlatintextfont{Noto Serif} 


\pagestyle{fancy}     % Enable fancy headers
\fancyhf{}            % Clear default header/footer
\renewcommand{\headrulewidth}{0pt} % Disable default header line

\fancyhead[L]{\rule{\textwidth}{1pt}} % Manually add one line
\fancyfoot[C]{\thepage} % Page number in the center of the footer

\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}
\definecolor{lightcyan}{rgb}{0.84,1,1}
\definecolor{lightgreen}{rgb}{0.64,1,0.71}
\definecolor{lightred}{rgb}{1,0.7,0.71}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\ExplSyntaxOn
\NewDocumentCommand{\ReverseWords}{m}
{
	\seq_set_split:Nnn \l_tmpa_seq { ~ } { #1 } % Split words by spaces
	\seq_reverse:N \l_tmpa_seq % Reverse the order of words
	\seq_use:Nn \l_tmpa_seq { ~ } % Join words with spaces and output
}
\ExplSyntaxOff

\begin{document}
	\author{ مجتبی ملائی \\ ۴۰۱۳۱۳۸۳ }
	\title{ \huge { تکلیف سوم}}
	\date{}
	\maketitle
	
\section{}
\begin{enumerate}
	\item 
یکی از چالش‌های اصلی در استفاده از شبکه‌های عصبی معمولی برای پردازش تصاویر، تعداد بسیار زیاد پارامترهاست. در این نوع شبکه‌ها، هر نورون به‌صورت کامل به تمام نورون‌های لایه قبلی متصل است، که در مورد تصاویر با ابعاد بالا، منجر به تولید میلیون‌ها پارامتر می‌شود. به‌عنوان مثال، یک تصویر رنگی با ابعاد 224×224 دارای بیش از 150 هزار ورودی است، که اتصال آن به حتی یک لایه پنهان متوسط باعث افزایش نمایی تعداد وزن‌ها می‌شود. این موضوع نه تنها بار محاسباتی را به‌طرز چشم‌گیری افزایش می‌دهد، بلکه باعث دشواری در آموزش شبکه، نیاز به منابع پردازشی قوی، و افزایش ریسک overfitting می‌شود.
\item 
دومین مشکل اساسی شبکه‌های عصبی معمولی در پردازش تصاویر، عدم توانایی در بهره‌برداری از ساختار فضایی و ویژگی‌های محلی تصویر است. این شبکه‌ها داده‌های ورودی را به‌صورت یک بردار یک‌بعدی در نظر می‌گیرند و هیچ توجهی به موقعیت مکانی پیکسل‌ها یا الگوهای محلی (نظیر لبه‌ها و بافت‌ها) ندارند. در نتیجه، اطلاعات مکانی که برای تحلیل تصویر بسیار حیاتی هستند، نادیده گرفته می‌شوند. این محدودیت باعث می‌شود که مدل نتواند به‌درستی ویژگی‌های بصری را استخراج کرده و درک عمیقی از محتوای تصویر به‌دست آورد، در حالی‌که شبکه‌های کانولوشنی با بهره‌گیری از فیلترهای محلی و اشتراک وزن، این ساختارهای فضایی را به‌شکل موثرتری مدل‌سازی می‌کنند.
\end{enumerate}

\section{}
در بینایی کامپیوتر کلاسیک، ویژگی‌ها به‌صورت دستی و با استفاده از الگوریتم‌ هاو یا فیلتر های از پیش تعریف‌ شده استخراج می‌شوند. این ویژگی‌ها بر اساس دانش انسان و تجربه طراحی می‌شوند و سپس به یک مدل یادگیرنده ساده برای طبقه‌بندی یا شناسایی داده می‌شوند. این فرآیند نیازمند تخصص انسانی و تنظیمات دقیق است و در مواجهه با داده‌های متنوع یا نویزدار ممکن است عملکرد ضعیفی داشته باشد.

در مقابل، روش‌های CNN ویژگی‌ها را به‌صورت خودکار و سلسله‌مراتبی از داده خام یاد می‌گیرند. شبکه با یادگیری از داده‌های برچسب‌دار، فیلترهایی را آموزش می‌بیند که به‌صورت لایه‌به‌لایه از ویژگی‌های ساده (مانند لبه‌ها) تا ویژگی‌های پیچیده (مانند اشیاء یا چهره‌ها) را استخراج می‌کنند. این روش نه‌تنها نیاز به طراحی دستی ویژگی‌ها را حذف می‌کند، بلکه باعث افزایش دقت، انعطاف‌پذیری و عملکرد در وظایف مختلف بینایی کامپیوتر شده است.

در واقع در روش های CNN تنظیم این فیلتر هاتوسط یادگیری خود مدل تعیین می‌شوند نه براساس طراحی انسان ها. 
\section{}

استفاده از \ReverseWords{Max Pooling} به جای \ReverseWords{Average Pooling} در شبکه‌های عصبی کانولوشنی معمولاً ترجیح داده می‌شود، زیرا \ReverseWords{Max Pooling} ویژگی‌های برجسته‌تر و مهم‌تر تصویر را بهتر حفظ می‌کند و در عمل به بهبود عملکرد مدل در وظایف بینایی کامپیوتر منجر می‌شود.

در \ReverseWords{Max Pooling}، بیشترین مقدار در یک ناحیه مشخص انتخاب می‌شود، که به معنای حفظ قوی‌ترین فعال‌سازی‌ها \ReverseWords{(strongest activations)} است. این فعال‌سازی‌ها معمولاً نشان‌دهنده وجود ویژگی‌های مهم مانند لبه‌ها، گوشه‌ها یا بافت‌های خاص هستند. در مقابل، \ReverseWords{Average Pooling} میانگین مقادیر را محاسبه می‌کند که می‌تواند اطلاعات کلیدی را با مقادیر غیرمهم ترکیب کرده و در نتیجه ویژگی‌های برجسته را "تضعیف" کند. بنابراین، \ReverseWords{Max Pooling} با تمرکز روی برجسته‌ترین نشانه‌ها در تصویر، به استخراج ویژگی‌هایی کمک می‌کند که برای تشخیص و طبقه‌بندی دقیق‌تر حیاتی هستند.

در واقع، \ReverseWords{Max Pooling} همچنین نوعی مقاومت در برابر نویز ایجاد می‌کند؛ زیرا با نادیده گرفتن مقادیر ضعیف‌تر، تأثیر ناهنجاری‌های کوچک یا تغییرات جزیی در تصویر کاهش می‌یابد. در مجموع، \ReverseWords{Max Pooling} باعث حفظ اطلاعات مهم، کاهش ابعاد، افزایش کارایی محاسباتی، و بهبود قابلیت تعمیم شبکه می‌شود، که دلایل اصلی ترجیح آن بر \ReverseWords{Average Pooling} در اکثر کاربردهای عملی است.
\section{}
سایز خروجی لایه کانولوشنی:
\begin{align*}
n_h[l] &= \lfloor \frac{n_h[l-1] - 2p - f_h}{s} \rfloor +1 \\
n_w[l] &= \lfloor \frac{n_w[l-1] - 2p - f_w}{s} \rfloor +1 \\
n_c[l] &= K	
\end{align*}
تعداد پارامتر های فیلتر های کانولوشنی:
\begin{align*}
	( n_h[l] * n_w[l] * n_c[l] + 1 ) * K 
\end{align*}
سایز خروجی لایه های پولینگ:
\begin{align*}
	n_h[l] &= \lfloor \frac{n_h[l-1]  - f_h}{s} \rfloor +1 \\
	n_w[l] &= \lfloor \frac{n_w[l-1]  - f_w}{s} \rfloor +1 \\
	n_c[l] &= n_c[l-1]	
\end{align*}
تعداد پارامتر های FC\lr{:}
\begin{align*}
	(1 + n[l-1]) * n[l]
\end{align*}

\begin{enumerate}
	\item لایه کانولوشنی اول:
	\begin{align*}
		n &= \lfloor \frac{28 - 2 \times 0 - 5}{1} \rfloor + 1 = 24 \\
		\text{size} &= 24 \times 24 \times 6 \\ 
		\text{params} &= (5 \times 5 \times 1 + 1) \times 6 = 156
	\end{align*}
	
	\item لایه \ReverseWords{avg pooling} اول:
	\begin{align*}
		n &= \lfloor \frac{24 - 2}{2} \rfloor + 1 = 12 \\
		\text{size} &= 12 \times 12 \times 6 \\ 
		\text{params} &= 0
	\end{align*}
	
	\item لایه کانولوشنی دوم:
	\begin{align*}
		n &= \lfloor \frac{12 - 2 \times 0 - 5}{1} \rfloor + 1 = 8 \\
		\text{size} &= 8 \times 8 \times 16 \\
		\text{params} &= (5 \times 5 \times 6 + 1) \times 16 = 2416
	\end{align*}
	
	\item لایه \ReverseWords{avg pooling} دوم:
	\begin{align*}
		n &= \lfloor \frac{8 - 2}{2} \rfloor + 1 = 4 \\
		\text{size} &= 4 \times 4 \times 16 = 256 \\
		\text{params} &= 0
	\end{align*}
	
	\item لایه \lr{FC1}:
	\begin{align*}
		\text{input} &= 4 \times 4 \times 16 = 256 \\
		\text{size} &= 128 \\
		\text{params} &= (256 + 1) \times 128 = 32,896
	\end{align*}
	
	\item لایه \lr{FC2}:
	\begin{align*}
		\text{input} &= 128 \\
		\text{size} &= 64 \\
		\text{params} &= (128 + 1) \times 64 = 8,256
	\end{align*}
	
	\item لایه خروجی (\lr{softmax}):
	\begin{align*}
		\text{input} &= 64 \\
		\text{size} &= 10 \\
		\text{params} &= (64 + 1) \times 10 = 650
	\end{align*}
	
	\item مجموع پارامترها:
	\begin{align*}
		\text{Conv layers} &= 156 + 2416 = 2,572 \\
		\text{FC layers} &= 32,896 + 8,256 + 650 = 41,802 \\
		\text{Total} &= \boxed{44,374}
	\end{align*}
\end{enumerate}


\section{}
خیر، نمونه‌برداری تصادفی ساده از دیتاست برای تقسیم آن به زیرمجموعه‌های آموزش، ارزیابی و آزمایش، همواره کفایت نمی‌کند؛ زیرا در بسیاری از موارد ممکن است این روش منجر به تقسیم نامتوازن یا نامناسب داده‌ها شود که بر عملکرد مدل اثر منفی می‌گذارد.

در شرایطی که توزیع کلاس‌ها نامتوازن است (یعنی برخی کلاس‌ها نمونه‌های بسیار بیشتری نسبت به سایر کلاس‌ها دارند)، نمونه‌برداری تصادفی ممکن است باعث شود که برخی کلاس‌ها در یکی از زیرمجموعه‌ها (مثلاً تست یا validation) به‌درستی نمایندگی نشوند یا حتی کاملاً حذف شوند. این امر باعث می‌شود مدل نتواند به‌درستی یاد بگیرد یا ارزیابی دقیق انجام دهد. به همین دلیل، معمولاً از روش‌های دقیق‌تری مانند نمونه‌برداری طبقه‌بندی‌شده \ReverseWords{ (stratified sampling)} استفاده می‌شود تا نسبت کلاس‌ها در هر زیرمجموعه حفظ شود.

همچنین در برخی کاربردها (مثلاً تشخیص چهره یا داده‌های زمانی)، لازم است نمونه‌های وابسته به هم (مانند فریم‌های یک ویدئو یا تصاویر یک فرد) در یک زیرمجموعه باقی بمانند تا از نشت اطلاعات \ReverseWords{(data leakage)} جلوگیری شود. در این موارد نیز نمونه‌برداری تصادفی ساده ناکافی و حتی خطرناک است. در نتیجه، تقسیم‌بندی مؤثر دیتاست نیازمند توجه به ساختار داده، توزیع کلاس‌ها و هدف مدل‌سازی است و صرفاً تکیه بر تصادفی‌سازی کافی نیست.
\section{}
خیر، اضافه کردن یک مقدار ثابت به تمام ورودی‌های تابع Softmax خروجی آن را تغییر نمی‌دهد. دلیل این موضوع به ویژگی نرمال‌سازی نسبی در تابع Softmax بازمی‌گردد. 

\textbf{اثبات:}
برای یک بردار ورودی $\mathbf{z} = [z_1, z_2, \ldots, z_n]$، خروجی Softmax به‌صورت زیر تعریف می‌شود:

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

افزودن مقدار ثابت $c$ به همه ورودی‌ها:

فرض کنید بردار جدید $\mathbf{z'} = [z_1 + c, z_2 + c, \ldots, z_n + c]$ باشد. حال خروجی Softmax برای $z_i + c$ برابر است با:

$$
\text{Softmax}(z_i + c) = \frac{e^{z_i + c}}{\sum_{j=1}^{n} e^{z_j + c}} = \frac{e^c \cdot e^{z_i}}{e^c \cdot \sum_{j=1}^{n} e^{z_j}} = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

همان‌طور که مشاهده می‌شود، $e^c$ از صورت و مخرج ساده می‌شود، بنابراین خروجی تغییر نمی‌کند.

\section{}
توصیه می‌شود که در مدل‌های دارای Dropout از نمودار هزینه (Loss) بر حسب ایپاک (Epoch) برای تعیین نرخ یادگیری به‌تنهایی استفاده نشود، زیرا نوسانات ذاتی Dropout باعث ناپایداری در مقدار هزینه می‌شود و ممکن است اطلاعات گمراه‌کننده درباره روند یادگیری ارائه دهد.

در فرآیند ،Dropout در هر بار  \ReverseWords{،forward pass} زیرمجموعه‌ای تصادفی از نورون‌ها غیرفعال می‌شوند. این امر باعث می‌شود که مقدار تابع هزینه از یک ایپاک به ایپاک دیگر رفتار نوسانی و پر از جهش‌های تصادفی داشته باشد. در نتیجه، نمودار هزینه ممکن است دارای افت‌وخیزهای زیاد باشد که تشخیص روند کلی همگرایی یا واگرایی مدل را دشوار می‌کند. این امر می‌تواند موجب انتخاب نامناسب نرخ یادگیری شود، چرا که تحلیل‌گر ممکن است نوسانات را به نرخ یادگیری نامطلوب نسبت دهد در حالی که منشأ آن Dropout است.

\section{}
\begin{enumerate}
	\item  \textbf{کاهش یا تغییر ابعاد کانال‌ها:}
	کانولوشن ۱×۱ تنها بر بعد عمقی (تعداد کانال‌ها) اثر می‌گذارد و بدون تغییر در ابعاد مکانی (طول و عرض تصویر)، می‌تواند تعداد کانال‌ها را کاهش یا افزایش دهد. این قابلیت برای کاهش پیچیدگی محاسباتی یا افزایش قدرت بازنمایی در لایه‌های خاص بسیار مفید است. به عنوان مثال، در U-Net از کانولوشن ۱×۱ برای تطبیق تعداد کانال‌ها قبل از اتصال لایه‌های encoder و decoder استفاده می‌شود.
	\item \textbf{افزایش غیرخطی بودن و ترکیب ویژگی‌ها:}
	با اعمال فیلتر ۱×۱ روی چندین کانال، مدل قادر است بین ویژگی‌های مختلف ترکیب‌های غیرخطی جدیدی یاد بگیرد. این عملیات شبیه به یک لایه Fully Connected بر روی هر موقعیت مکانی (pixel) است و به مدل اجازه می‌دهد تا وابستگی‌های میان‌کانالی را در همان مکان بررسی کند، که برای استخراج ویژگی‌های سطح بالا مفید است.
\end{enumerate}

\section{}
\subsection{نشانه ها}
\begin{enumerate}
	\item \textbf{اختلاف زیاد بین دقت آموزش و دقت اعتبارسنجی:}
	مدل در داده‌های آموزش عملکرد بسیار خوبی دارد (خطای کم یا دقت بالا)، اما در داده‌های اعتبارسنجی (Validation) عملکرد ضعیفی نشان می‌دهد. این اختلاف نشان‌دهنده یادگیری بیش از حد ویژگی‌های خاص داده‌های آموزش است، نه الگوهای عمومی.
	\item \textbf{کاهش پیوسته خطای آموزش، اما ثابت یا افزایش‌یافتن خطای اعتبارسنجی:}
	   اگر با گذشت ایپاک‌ها، خطای آموزش کاهش یابد ولی خطای اعتبارسنجی بهبود نیابد یا حتی افزایش یابد، این نشانه واضحی از overfitting است.
	\end{enumerate}
\subsection{دو تکنیک کاربردی برای کاهش :Overfitting}
\begin{enumerate}
	\item \textbf{استفاده از Regularization مانند Dropout یا \ReverseWords{:L2 Regularization}}
	\begin{itemize}
		\item \textbf{:Dropout} به‌صورت تصادفی برخی نورون‌ها را در طول آموزش غیرفعال می‌کند، که از وابستگی بیش‌ازحد مدل به مسیرهای خاص جلوگیری می‌کند. در CNN ها معمولاً در لایه‌های Fully Connected استفاده می‌شود.
		\item \textbf{L2 :Regularization} با اضافه‌کردن جریمه به وزن‌های بزرگ، مدل را وادار می‌کند تا پارامترهای ساده‌تر و عمومی‌تری بیاموزد.
	\end{itemize}
	\item \textbf{افزایش داده‌ها از طریق \ReverseWords{:Data Augmentation}}
	با ایجاد تغییرات تصادفی در تصاویر آموزشی (چرخش، برش، مقیاس‌گذاری، وارونه‌سازی، تغییر روشنایی و غیره)، داده‌های متنوع‌تری ایجاد می‌شود که کمک می‌کند مدل ویژگی‌های پایدار و عمومی‌تری یاد بگیرد و از حفظ ویژگی‌های خاص و جزئی تصویر اصلی اجتناب کند.
\end{enumerate}
\section{}
در مدل‌های یادگیری عمیق، خروجی Softmax معمولاً به‌عنوان توزیع احتمال روی کلاس‌ها تفسیر می‌شود. اما در عمل، این احتمال‌ها اغلب overconfident هستند، یعنی مدل حتی در شرایط عدم قطعیت، احتمال بسیار بالایی برای یک کلاس خاص اعلام می‌کند. این موضوع باعث کاهش قابلیت اطمینان مدل در سیستم‌های حساس به تصمیم‌گیری می‌شود.
برای کالیبره‌سازی (Calibrating) بهتر خروجی، از پارامتر دمایی\ReverseWords{(Temperature Scaling) } استفاده می‌شود. در این روش، خروجی logits مدل (بردار قبل از (Softmax بر یک پارامتر T > 0 تقسیم می‌شود:
$$
\text{Softmax}_T(z_i) = \frac{e^{z_i / T}}{\sum_{j} e^{z_j / T}}
$$
\begin{itemize}
	\item 
	وقتی $T = 1$: خروجی معمولی Softmax تولید می‌شود.
	\item 
	وقتی $T > 1$: توزیع نرم‌تر می‌شود؛ مدل با اطمینان کمتری پیش‌بینی می‌کند.
	\item 
	وقتی $T < 1$: توزیع تیزتر می‌شود؛ مدل با اطمینان بیشتری تصمیم می‌گیرد.
\end{itemize}






در مرحله کالیبراسیون، مقدار $T$ روی مجموعه اعتبارسنجی (Validation) بهینه‌سازی می‌شود تا فاصله بین توزیع پیش‌بینی‌شده و واقعیت آماری (مثلاً با معیارهایی مانند \ReverseWords{ (Expected Calibration Error } کمینه شود.

\textbf{مثال کاربردی: سیستم‌های تصمیم‌گیری در پزشکی}

در یک سیستم تشخیص پزشکی با استفاده از شبکه عصبی کانولوشنی، خروجی مدل ممکن است احتمال "سرطان بودن" یا "سالم بودن" یک بافت را گزارش دهد. اگر مدل بدون کالیبره‌سازی Softmax مقدار 0.98 برای "سرطان" تولید کند، ولی در واقعیت تنها در 80٪ موارد این پیش‌بینی درست باشد، سیستم تصمیم‌گیر ممکن است به‌شدت گمراه شود. با اعمال Temperature Scaling، این احتمال ممکن است به مقدار واقع‌گرایانه‌تری مانند 0.80 تنظیم شود، که منجر به تصمیم‌گیری قابل اعتمادتر توسط پزشک یا سیستم اتوماسیون می‌شود.


\section{}

در معماری‌های بسیار عمیق CNN، دو چالش اصلی وجود دارد: مشکل ناپدید شدن/انفجار گرادیان‌ها \ReverseWords{(vanishing/exploding gradients)} و مشکل افت عملکرد با افزایش عمق شبکه \ReverseWords{.(degradation problem)} به‌طور خلاصه، هنگامی که شبکه‌ها عمیق می‌شوند، گرادیان‌های backpropagation در لایه‌های اولیه به‌طور تدریجی کاهش یا افزایش می‌یابند که فرآیند یادگیری را ناامن و ناپایدار می‌سازد. علاوه بر این، افزایش عمق منجر به کاهش عملکرد شبکه در برخی موارد می‌شود چرا که شباهت‌های اطلاعاتی و وابستگی‌های پیچیده بین لایه‌ها به‌طور ناکارآمد منتقل می‌گردد و مدل نتواند الگوهای کلی را به‌طور بهینه استخراج کند.

تکنیک‌هایی مانند \ReverseWords{skip connections} در معماری ResNet با ارائه مسیرهای معادلی \ReverseWords{(identity mapping)} برای انتقال، این مشکلات را برطرف می‌کنند. این اتصالات با ایجاد ارتباط مستقیم بین لایه‌های اولیه و نهایی شبکه، اجازه می‌دهند تا گرادیان‌ها بدون از دست دادن اطلاعات اصلی به عقب منتقل شوند؛ به عبارت دیگر، مدل قادر است وابستگی‌های عمیق‌تری را بدون ریسک از دست دادن اطلاعات مفید، یاد بگیرد. از منظر عملکردی، این ساختار باعث می‌شود که بهبودهای قابل توجهی در همگرایی، دقت و تعمیم‌پذیری مدل حاصل شود، به گونه‌ای که ساختارهای بسیار عمیق بدون مشکلات کاهش عملکرد، به‌راحتی قابل آموزش باشند.

\section{ بخش الگوریتم ژنتیک}

\subsection{}
	\begin{enumerate}[label=\textbf{\Alph*)}]
	\item یک کوروموزم به طول تعداد دانشجویان در نظر میگیریم. در این کروموزوم خانه $i$ نشان دهنده انتخاب دانشجوی $i$ام است و مقدار آن نشان دهنده پروژه آن است.
	
	$\text{Chromosome} = [Pj_1,Pj_2,Pj_2,...,Pj_s]$
	
به عنوان مثال: 
	$\text{Chromosome} = [1,7,9,2,3]$
	یعنی دانشجوی اول پروژه اول، دانشجوی دوم پروژه هفتم و ... 
	
	\item 
 ‎  ‎
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{دانشجو} & \textbf{P 1} & \textbf{P 2} & \textbf{P 3} & \textbf{P 4} & \textbf{P 5} & \textbf{P 6} & \textbf{P 7} & \textbf{P 8} & \textbf{P 9} & \textbf{P 10} & \textbf{P 11} & \textbf{P 12} & \textbf{P 13} \\
		\hline
		1 & 1000 & 1 & 1000 & 1000 & 2 & 1000 & 1000 & 3 & 1000 & 4 & 1000 & 1000 & 5 \\
		2 & 2 & 1 & 1000 & 1000 & 1000 & 3 & 1000 & 1000 & 4 & 5 & 1000 & 1000 & 1000 \\
		3 & 1000 & 2 & 1000 & 1 & 4 & 1000 & 5 & 1000 & 1000 & 1000 & 3 & 1000 & 1000 \\
		4 & 1000 & 1000 & 3 & 1000 & 1000 & 1 & 1000 & 2 & 5 & 4 & 1000 & 1000 & 1000 \\
		5 & 1 & 4 & 1000 & 1000 & 3 & 1000 & 2 & 1000 & 1000 & 1000 & 1000 & 1000 & 5 \\
		6 & 1000 & 1000 & 1000 & 2 & 1000 & 3 & 1000 & 1000 & 1 & 1000 & 5 & 4 & 1000 \\
		7 & 3 & 1000 & 4 & 1000 & 1000 & 1000 & 5 & 1 & 1000 & 2 & 1000 & 1000 & 1000 \\
		8 & 1000 & 3 & 1000 & 5 & 1 & 2 & 1000 & 1000 & 1000 & 1000 & 4 & 1000 & 1000 \\
		9 & 1000 & 1000 & 5 & 1000 & 1000 & 1000 & 1 & 2 & 4 & 1000 & 1000 & 1000 & 3 \\
		\hline
	\end{tabular}
	\caption{جدول انتخاب دانشجویان}
\end{table}
مقدار جریمه در این مثال ۱۰۰۰ در نظر گرفته شده است. 
 \textbf{جمعیت اولیه}
 
 \[
 \begin{array}{c|ccccccccc}
 	\textbf{Chromosome} & S_1 & S_2 & S_3 & S_4 & S_5 & S_6 & S_7 & S_8 & S_9 \\
 	\hline
 	1 & 2 & 1 & 2 & 1 & 1 & 1 & 1 & 1 & 1 \\
 	2 & 5 & 3 & 4 & 2 & 7 & 4 & 10 & 6 & 9 \\
 	3 & 8 & 9 & 11 & 3 & 5 & 6 & 1 & 2 & 12 \\
 	4 & 1 & 6 & 5 & 10 & 2 & 12 & 3 & 11 & 13 \\
 	5 & 10 & 12 & 7 & 9 & 13 & 11 & 7 & 4 & 3 \\
 \end{array}
 \]
	\item 
	\begin{itemize}
		\item 
اگر دو دانشجو یک پروژه را بردارند باید برای آن جریمه تعریف کنیم. (مثلا ۱۰۰۰)
\item 
اگر دانشجویی نیز پروژه ای دارد که در ۵ اولولیت آن نبوده باید برای آن جریمه تعریف کنیم. (قبلا تعریف شده است) 
\item 
حداکثر جریمه ممکن برابر است با وقتی که همه دانشجویان یک پروژه یکسان بردارند که در اولویت هیچ کدام نیست.  $|S| * 1000 * 2$
\item همچنین باید تلاش شود تا دانشجویان اولویت های بالا تر خود برسند.
\item 
تابع dupcnt تکراری ها را میشمارد.   
	\end{itemize}
	تابع fitness زیر اینکار را انجام میدهد:
	
	$f(c) = \frac{1}{ \sum_{i} r_{ij} + dupcnt(c) *1000} * 9*2000 $
	
این تابع همه شرایط لازم را دارد. و خروجی آن بین ۱ تا ۲۰۰۰ برای این مسئله می‌باشد.
\begin{latin}
	\begin{lstlisting}[language=python]
		student_preferences = [
		[1000, 1, 1000, 1000, 2, 1000, 1000, 3, 1000, 4, 1000, 1000, 5],
		[2, 1, 1000, 1000, 1000, 3, 1000, 1000, 4, 5, 1000, 1000, 1000],
		[1000, 2, 1000, 1, 4, 1000, 5, 1000, 1000, 1000, 3, 1000, 1000],
		[1000, 1000, 3, 1000, 1000, 1, 1000, 2, 5, 4, 1000, 1000, 1000],
		[1, 4, 1000, 1000, 3, 1000, 2, 1000, 1000, 1000, 1000, 1000, 5],
		[1000, 1000, 1000, 2, 1000, 3, 1000, 1000, 1, 1000, 5, 4, 1000],
		[3, 1000, 4, 1000, 1000, 1000, 5, 1, 1000, 2, 1000, 1000, 1000],
		[1000, 3, 1000, 5, 1, 2, 1000, 1000, 1000, 1000, 4, 1000, 1000],
		[1000, 1000, 5, 1000, 1000, 1000, 1, 2, 4, 1000, 1000, 1000, 3]
		]
		chromosomes = [
		[2, 1, 2, 1, 1, 1, 1, 1, 1],
		[5, 3, 4, 2, 7, 4, 10, 6, 9],
		[8, 9, 11, 3, 5, 6, 1, 2, 12],
		[1, 6, 5, 10, 2, 12, 3, 11, 13],
		[10, 12, 7, 9, 13, 11, 7, 4, 3]
		]
		def fitness(chromosome, preferences):
			total_cost = 0
			duplicate_penalty = 0
			seen = set()
		
			for student_index, project in enumerate(chromosome):
				project_index = project - 1
				cost = preferences[student_index][project_index]
				total_cost += cost
			
				if project in seen:
					duplicate_penalty += 1
				else:
					seen.add(project)
			
			penalty = duplicate_penalty * 1000
			adjusted_cost = total_cost + penalty
			
			fitness_value = (1 / adjusted_cost) * 9 * 2000
			return fitness_value
		
		# Calculate fitness for all chromosomes
		fitness_values = [fitness(ch, student_preferences) for ch in chromosomes]
		for i, fval in enumerate(fitness_values, start=1):
			print(f"Fitness of Chromosome {i}: {fval:.4f}")
		
	\end{lstlisting}
	output:
	\begin{lstlisting}
		Fitness of Chromosome 1: 1.6350
		Fitness of Chromosome 2: 5.9701
		Fitness of Chromosome 3: 17.5610
		Fitness of Chromosome 4: 17.4757
		Fitness of Chromosome 5: 8.8279\end{lstlisting}

\end{latin}
	\item 
	\begin{itemize}
		\item 
		ترکیب کروموزم سوم و چهارم
		$p3 = [8, 9, 11, 3, 5, 6, 1, 2, 12]\\ $
		$p4 = [1, 6, 5, 10, 2, 12, 3, 11, 13]\\$
		اگر از \ReverseWords{uniform crossover} استفاده کنیم:
		$c1 = [8, 6, 5, 3, 2, 12, 1, 11, 13]\rightarrow 580.6451\\$
		$c2 = [1, 9, 11, 10, 5, 6, 3, 2, 12 ] \rightarrow 8.8932\\$
		\item 
				ترکیب کروموزم دوم و سوم
		$p2 = [5, 3, 4, 2, 7, 4, 10, 6, 9] \\$
		$p3 = [8, 9, 11, 3, 5, 6, 1, 2, 12]\\$
		اگر از \ReverseWords{uniform crossover} استفاده کنیم:
		$c3 = [8, 3, 11, 2, 5, 4, 7, 6, 9] \rightarrow 8.9020 \\$
		$c4 = [5, 9, 4, 3, 7, 6, 10, 2, 12] \rightarrow 17.6470 \\$
	\end{itemize}
	\item 
	برگزیدگان:
	\begin{latin}
			\begin{tabular}{|c|c|}
			\hline
			c1 & 580.6451 \\ \hline 
			c4 & 17.6470 \\ \hline
			p3 & 17.5610 \\ \hline 
			p4 & 17.4757 \\ \hline
			c3 & 8.9020 \\ \hline
		\end{tabular}
	\end{latin}

	برازندگی کل نسل قبلی : $51.4697$
	
	برازندگی کل نسل جدید : $642.2308$
\end{enumerate}

\subsection{}


\begin{enumerate}[label=\textbf{\Alph*)}]
	\item 
\textbf{نمایش مناسب برای کروموزوم‌ها:}
یک کروموزوم می‌تواند به‌صورت یک رشته از شناسه‌های مشتریان به همراه شناسه وسیله نقلیه در قالب لیست‌های مجزا نمایش یابد. برای مثال:

\[
\text{Chromosome} = \left[ [v_1: c_3 \rightarrow c_5 \rightarrow c_1], [v_2: c_2 \rightarrow c_6], [v_3: c_4] \right]
\]

که در آن $v_i$ نمایانگر وسیله نقلیه و $c_j$ نشان‌دهنده مشتری $j$ام است. این نمایش هم اختصاص مشتریان به وسایل نقلیه و هم ترتیب ملاقات را مشخص می‌کند.

\item \textbf{تعریف اپراتور قیاسی با جریمه}
تابع هزینه به‌صورت زیر تعریف می‌شود:

\[
f = \sum_{i=1}^{m_1} d_i + \lambda_1 \cdot P_{\text{capacity}} + \lambda_2 \cdot P_{\text{timewindow}}
\]

که:
\begin{itemize}
	\item $d_i$ مسافت طی‌شده توسط وسیله نقلیه $i$ است.
	\item $P_{\text{capacity}}$ مجموع جریمه‌های ناشی از تجاوز ظرفیت وسایل نقلیه است.
	\item $P_{\text{timewindow}}$ مجموع جریمه‌های ناشی از تخطی پنجره زمانی مشتریان است.
	\item $\lambda_1$ و $\lambda_2$ ضرایب جریمه هستند.
\end{itemize}

\item \textbf{طراحی عملگرهای Crossover و Mutation}

\textbf{:Crossover}

استفاده از عملگر \textbf{\ReverseWords{Route-Based Crossover (RBX)}}:

\begin{itemize}
	\item یک یا چند مسیر (route) تصادفی از والد اول انتخاب شده و بدون تغییر به فرزند منتقل می‌شوند.
	\item مشتریانی که در این مسیرها نیستند، از والد دوم و به ترتیبی که در والد دوم آمده‌اند، در جای خالی باقی‌مانده قرار می‌گیرند به‌گونه‌ای که تکراری نباشند.
\end{itemize}

\textbf{مثال}:

\begin{latin}
	\(
	\text{Parent 1: } [[v_1: c_1 \rightarrow c_2], [v_2: c_3 \rightarrow c_4]] \\
	\text{Parent 2: } [[v_1: c_4 \rightarrow c_1], [v_2: c_2 \rightarrow c_3]]
	\)
\end{latin}

فرض کنید مسیر \([v_1: c_1 \rightarrow c_2]\) از والد اول انتخاب شود. مشتریان باقی‌مانده \((c_3, c_4)\) طبق ترتیب والد دوم به فرزند افزوده می‌شوند، نتیجه:

\[
\text{Offspring: } [[v_1: c_1 \rightarrow c_2], [v_2: c_4 \rightarrow c_3]]
\]

\textbf{:Mutation}

استفاده از دو عملگر جهش:

\begin{itemize}
	\item \textbf{Swap Mutation:} تعویض موقعیت دو مشتری در یک مسیر یا بین مسیرهای مختلف.
	\item \textbf{Relocate Mutation:} جابجایی یک مشتری از مسیر فعلی‌اش به موقعیتی دیگر در همان مسیر یا مسیر متفاوت.
\end{itemize}

\textbf{مثال Swap}:

\[
[v_1: c_1 \rightarrow c_2 \rightarrow c_3] \Rightarrow [v_1: c_1 \rightarrow c_3 \rightarrow c_2]
\]

\textbf{مثال Relocate}:

\[
[v_1: c_1 \rightarrow c_2], [v_2: c_3] \Rightarrow [v_1: c_2], [v_2: c_3 \rightarrow c_1]
\]

\item \textbf{انتخاب والد و بازمانده}

برای انتخاب والدین از روش \ReverseWords{ (Tournament Selection)} استفاده می‌شود:

\begin{itemize}
	\item $k$ کروموزوم به‌صورت تصادفی انتخاب شده و بهترین آن‌ها بر اساس تابع هزینه به‌عنوان والد انتخاب می‌شود.
\end{itemize}

برای انتخاب بازماندگان از روش \textbf{Elitism} استفاده می‌شود:

\begin{itemize}
	\item درصدی از بهترین کروموزوم‌های نسل فعلی مستقیماً به نسل بعد منتقل می‌شوند.
	\item بقیه جمعیت از بین فرزندان تولیدشده انتخاب می‌گردند.
\end{itemize}

\item \textbf{ شرط توقف}

شرایط توقف می‌تواند یکی از موارد زیر باشد:

\begin{itemize}
	\item رسیدن به تعداد نسل معین $G_{\text{max}}$.
	\item عدم بهبود تابع هزینه برای $k$ نسل متوالی.
	\item رسیدن به مقدار حداقل مطلوب برای تابع هزینه.
\end{itemize}
\end{enumerate}
\end{document}