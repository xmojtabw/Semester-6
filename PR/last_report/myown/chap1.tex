\chapter{تشخیص جعل عمیق\footnote{DeepFake} با استفاده از یادگیری عمیق\footnote{Deep Learning}}
\section{مقدمه}

با پیشرفت چشمگیر فناوری‌های هوش مصنوعی، تولید محتوای جعلی، به‌ویژه ویدیوهای جعلی موسوم به جعل عمیق، به یکی از چالش‌های جدی در حوزه‌های امنیتی، سیاسی و اجتماعی تبدیل شده است. این ویدیوها عمدتاً با بهره‌گیری از شبکه‌های مولد تخاصمی\footnote{GAN} تولید می‌شوند و می‌توانند ظاهر، حرکات و حتی صدای افراد را با دقت بالایی بازسازی کنند.

در آستانهٔ انتخابات ریاست‌جمهوری ایالات متحده در سال ۲۰۲۰، نگرانی‌ها دربارهٔ استفاده از جعل‌های عمیق برای تأثیرگذاری بر افکار عمومی افزایش یافت. در پاسخ، بستر‌هایی\footnote{Platforms} مانند فیسبوک\footnote{Facebook} و اینستاگرام\footnote{Instagram} سیاست‌هایی برای حذف محتوای دست‌کاری‌شده توسط هوش مصنوعی اتخاذ کردند. با این حال، تشخیص سریع و دقیق این ویدیوها مستلزم بهره‌گیری از سامانه‌های خودکار و مبتنی بر یادگیری عمیق است.

در این پژوهش، دو معماری قدرتمند در حوزهٔ بینایی ماشین یعنی \lr{Xception} و \lr{MobileNet} به‌منظور تشخیص خودکار ویدیوهای جعل عمیق به‌کار گرفته شده‌اند. این مدل‌ها با استفاده از مجموعه‌دادهٔ \lr{FaceForensics++} که شامل ویدیوهای جعلی تولیدشده با چهار روش رایج است، آموزش دیده‌اند. به‌منظور افزایش دقت نهایی، یک سازوکار رأی‌گیری میان خروجی این مدل‌ها طراحی و پیاده‌سازی شده است.

\section{کارهای مرتبط}

تکنیک‌های تولید جعل عمیق به‌طور کلی به دو دستهٔ اصلی \textbf{تعویض چهره\footnote{Face Swapping} و \textbf{بازسازی حالات چهره\footnote{Face Reenactment}}} تقسیم می‌شوند.

\subsection{تعویض چهره}

در این روش، چهرهٔ فردی با چهرهٔ فرد دیگری جایگزین می‌شود. یکی از اولین نمونه‌های کاربردی این تکنیک توسط کاربران فضای مجازی و با استفاده از ساختارهای سادهٔ رمزگذار-رمزگشا\footnote{Encoder-Decoder} ارائه شد. پس از آن، روش‌های پیشرفته‌تری همچون \lr{Faceswap-GAN} معرفی شدند که با افزودن ضررهای ادراکی و متضاد\footnote{Perceptual and Adversarial Losses}، دقت بازسازی در نواحی حساس مانند چشم و دهان را افزایش دادند. همچنین، تکنیک‌هایی مانند \lr{Fast Face-swap} با بهره‌گیری از انتقال سبک\footnote{Style Transfer} و شبکه‌های پیش‌آموزش‌دیده مانند \lr{VGG19}\footnote{VGG19 Pre-trained Network}، توانستند کنترل بیشتری بر ویژگی‌های ظاهری و محتوایی اعمال کنند.

\subsection{بازسازی حالات چهره}

در این دسته، چهرهٔ فرد در تصویر حفظ می‌شود اما حرکات و حالات آن از فرد دیگری الگوبرداری می‌شود. پروژه‌هایی مانند \lr{Face2Face} با بهره‌گیری از مدل‌سازی سه‌بعدی\footnote{3D Modeling} و تطبیق کلیدهای چهره\footnote{Facial Landmark Matching}، توانستند حرکات چهره را در زمان واقعی بازسازی کنند\footnote{Real-time Reconstruction}. همچنین \lr{NeuralTextures} با استفاده از بافت‌های آموخته‌شده\footnote{Learned Textures}، ناحیهٔ دهان را با دقت بالا بازسازی کرد.

\subsection{روش‌های تشخیص جعل عمیق}

در گذشته، روش‌های سنتی تشخیص جعل عمیق متکی بر ویژگی‌هایی همچون نرخ پلک‌زدن، ناهماهنگی نور، یا تحلیل جرم‌شناسی دیجیتال\footnote{Digital Forensics} بودند. اما این ویژگی‌ها به‌راحتی توسط الگوریتم‌های سازندهٔ جعل عمیق اصلاح‌پذیر هستند. در نتیجه، تمرکز پژوهش‌ها به سمت مدل‌های یادگیری عمیق، به‌ویژه شبکه‌های عصبی کانولوشنی\footnote{Convolutional Neural Networks (CNN)}، سوق یافته است. این مدل‌ها قادرند ویژگی‌های غیرقابل تشخیص برای انسان را از داده‌ها استخراج و تحلیل کنند.

\section{مجموعه‌داده}

برای آموزش و ارزیابی مدل‌ها، از مجموعه‌دادهٔ \lr{FaceForensics++} استفاده شد. این مجموعه شامل ۱۰۰۰ ویدیوی واقعی و ۴۰۰۰ ویدیوی جعلی تولیدشده با روش‌های \lr{Deepfakes}، \lr{Face2Face}، \lr{FaceSwap} و \lr{NeuralTextures} است. ویدیوها با نرخ فشرده‌سازی ۲۳ برابر\footnote{Compression Ratio of 23:1} و به‌صورت h264\footnote{H.264 Video Encoding} ذخیره شده‌اند. پردازش داده‌ها در زیرساخت \lr{SPARTAN} متعلق به دانشگاه ملبورن انجام شده است.

فرایند تقسیم داده‌ها به‌گونه‌ای انجام گرفت که از نشت داده\footnote{Data Leakage} بین بخش آموزش و آزمون جلوگیری شود. فریم‌ها از ویدیوها استخراج، چهره‌ها شناسایی و برش خورده، و نهایتاً به اندازهٔ مورد نیاز مدل‌ها تغییر اندازه داده شدند.

\section{روش‌شناسی}

مراحل اصلی این پژوهش عبارتند از:

\subsection{پیش‌پردازش و استخراج چهره‌ها}

با استفاده از ،OpenCV ویدیوها به فریم‌های جداگانه تقسیم و سپس چهره‌ها با طبقه‌بند Haar\footnote{Haar Cascade Classifier} شناسایی شدند. فقط بزرگ‌ترین ناحیهٔ چهره در هر فریم استخراج و به اندازهٔ موردنظر برای مدل‌ها تغییر داده شد: ۲۹۹×۲۹۹ برای \lr{Xception} و ۲۲۴×۲۲۴ برای \lr{MobileNet}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{screenshot002}
	\caption{روندنمای پیش‌پردازش }
	\label{fig:screenshot002}
\end{figure}

\subsection{معماری مدل‌ها}

\paragraph{:Xception} یک معماری مبتنی بر کانولوشن‌های جداشونده که همبستگی مکانی و کانالی را به‌طور مستقل مدل می‌کند. این شبکه از ۳۶ لایه کانولوشن تشکیل شده و در این پروژه از نسخهٔ پیاده‌سازی‌شده در Keras استفاده شده است.

\paragraph{:MobileNet} معماری سبک‌وزن با استفاده از کانولوشن‌های جداشونده برای استفاده در دستگاه‌های با منابع محدود. این مدل نیز از نسخهٔ آماده در Keras بهره گرفته است.

\subsection{تنظیم محیط آزمایش}

برای اجرای مراحل آموزش، محیطی مجازی با نسخه‌های سازگار از CUDA و TensorFlow در خوشهٔ\footnote{Cluster} \lr{SPARTAN} راه‌اندازی شد.

\subsection{تنظیمات آموزش}

هر مدل برای یک روش خاص جعل عمیق آموزش داده شد. تنظیمات آموزشی به‌شرح زیر است:

\begin{itemize}
	\item بهینه‌ساز\footnote{Optimizer}: \lr{Adam}
	\item تابع هزینه\footnote{Loss function}: \lr{Binary Cross-Entropy}
	\item اندازه دسته\footnote{Batch size}: ۳۲
	\item تعداد دوره‌ها\footnote{Epochs}: ۱۰
	\item بارگذاری تصاویر با مولد\footnote{Generator} جهت مدیریت حافظه
\end{itemize}

\section{نتایج}

نتایج نشان دادند که:
\begin{itemize}
	\item[-]
	  مدل \lr{Xception} دقتی بین ٪۹۳ تا ٪۹۸ ارائه داد.
	\item[-] 
	 مدل \lr{MobileNet} دقتی بین ٪۹۱ تا ٪۹۶ داشت.
	 \item[-]
	 هر دو مدل در مواجهه با ویدیوهای \lr{NeuralTextures} عملکرد ضعیف‌تری داشتند.
	 \item[-]
	 دقت مدل‌ها روی روش‌های دیگر جعل عمیق (که با آن‌ها آموزش ندیده بودند) به‌طور قابل توجهی کاهش یافت.
\end{itemize}

 

\section{سازوکار رأی‌گیری}

برای بهبود عملکرد نهایی، نتایج چهار مدل به‌صورت رأی‌گیری ترکیب شد. اگر هر مدلی بیش از نیمی از فریم‌های یک ویدیو را جعلی تشخیص می‌داد، رأی آن مدل «جعلی» محسوب می‌شد. سپس اگر حتی یک مدل رأی «جعلی» می‌داد، کل ویدیو به‌عنوان جعلی در نظر گرفته می‌شد. این سازوکار به‌ویژه در مواردی که فقط یک مدل قادر به شناسایی جعلی بودن ویدیو بود (مانند ویدیوهای \lr{NeuralTextures}) مؤثر واقع شد.

\section{بحث}

نتایج پژوهش نشان داد که آموزش مدل‌ها به‌صورت اختصاصی برای هر نوع جعل عمیق منجر به افزایش دقت می‌شود، اما توانایی تعمیم مدل‌ها محدود باقی می‌ماند. ترکیب مدل‌ها به‌صورت رأی‌گیری این مشکل را تا حدی برطرف کرده است، ولی همچنان در برابر روش‌های جدید (مانند \lr{StyleGAN2}) چالش‌هایی باقی می‌ماند.

\section{نتیجه‌گیری}

در این پژوهش، یک سامانهٔ تشخیص جعل عمیق مبتنی بر یادگیری عمیق با استفاده از دو معماری \lr{Xception} و \lr{MobileNet} طراحی و ارزیابی شد. با آموزش مدل‌ها روی مجموعه‌دادهٔ \lr{FaceForensics++} و طراحی سازوکار رأی‌گیری، عملکرد کلی سیستم در شناسایی انواع رایج جعل عمیق بهبود یافت.

پیشنهادهای آینده شامل:

\begin{itemize}
	\item آموزش مدل برای تکنیک‌های جدید مانند \lr{StyleGAN2}
	\item تحلیل ویژگی‌های ناحیه‌ای چهره
	\item استفاده از توالی فریم‌ها و اطلاعات زمانی
	\item توسعهٔ رابط کاربری ساده برای استفاده عمومی
\end{itemize}
